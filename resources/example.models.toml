default_provider = "openai-like"

# -------------------------------------------------
# OpenAI official endpoint
# -------------------------------------------------
[providers.openai]
id = "openai"
name = "OpenAI"
family = "openai"
base_url = "https://api.openai.com/v1"
api_key = ""
enabled = true
priority = 3
[[providers.openai.models]]
id = "gpt-4o"
name = "gpt-4o"
max_tokens = 16384
context_window = 128000
description = """{"code": 83, "write": 87, "reasoning": 86, "multimodal": 94, "cost": "input $5.00 output $20.00", "info": "Flagship multimodal model offering native voice, strong reasoning, and creative writing. Delivers low latency (~320 ms) and a 128K context window, suitable for conversational AI, content creation, and live multimodal interactions. Shares common LLM limitations such as hallucinations and uneven image understanding in narrow domains."}"""



# -------------------------------------------------
# Anthropic official endpoint
# -------------------------------------------------
[providers.anthropic]
id = "anthropic"
name = "Anthropic"
family = "anthropic"
base_url = "https://api.anthropic.com"
api_key = ""
enabled = true
priority = 4
[[providers.anthropic.models]]
id = "claude-sonnet-4-5-20250929"
name = "claude-sonnet-4-5-20250929"
max_tokens = 64000
context_window = 200000
description = """{"code": 88, "write": 82, "reasoning": 85, "multimodal": 74, "cost": "input $3.00 output $15.00", "info": "Mid-tier Anthropic model focused on code generation and debugging (~73 % SWE-bench). 200K context window and consistent instruction following. Multimodal limited to text-plus-image comprehension. Strong at code review and structured writing, but lags on open-ended creative tasks."}"""



# -------------------------------------------------
# DeepSeek official endpoint
# -------------------------------------------------
[providers.deepseek]
id = "deepseek"
name = "DeepSeek"
family = "deepseek"
base_url = "https://api.deepseek.com"
api_key = ""
enabled = true
priority = 5
[[providers.deepseek.models]]
id = "deepseek-reasoner"
name = "deepseek-reasoner"
max_tokens = 64000
context_window = 128000
description = """deepseek-reasoner is a reasoning model developed by DeepSeek. Before delivering the final answer, the model first generates a Chain of Thought (CoT) to enhance the accuracy of its responses. Our API provides users with access to the CoT content generated by deepseek-reasoner, enabling them to view, display, and distill it."""



# -------------------------------------------------
# OpenAI Like endpoint
# -------------------------------------------------
[providers.openai-like]
id = "openai-like"
name = "openai-like"
family = "openai"
base_url = ""
api_key = ""
enabled = true
priority = 1
[[providers.openai-like.models]]
id = "gpt-4o"
name = "gpt-4o"
max_tokens = 16384
context_window = 128000
description = """{"code": 83, "write": 87, "reasoning": 86, "multimodal": 94, "cost": "input $5.00 output $20.00", "info": "Flagship multimodal model offering native voice, strong reasoning, and creative writing. Delivers low latency (~320 ms) and a 128K context window, suitable for conversational AI, content creation, and live multimodal interactions. Shares common LLM limitations such as hallucinations and uneven image understanding in narrow domains."}"""

[[providers.openai-like.models]]
id = "claude-sonnet-4-5-20250929"
name = "claude-sonnet-4-5-20250929"
max_tokens = 64000
context_window = 200000
description = """{"code": 88, "write": 82, "reasoning": 85, "multimodal": 74, "cost": "input $3.00 output $15.00", "info": "Mid-tier Anthropic model focused on code generation and debugging (~73 % SWE-bench). 200K context window and consistent instruction following. Multimodal limited to text-plus-image comprehension. Strong at code review and structured writing, but lags on open-ended creative tasks."}"""

[[providers.openai-like.models]]
id = "deepseek-reasoner"
name = "deepseek-reasoner"
max_tokens = 64000
context_window = 128000
description = """deepseek-reasoner is a reasoning model developed by DeepSeek. Before delivering the final answer, the model first generates a Chain of Thought (CoT) to enhance the accuracy of its responses. Our API provides users with access to the CoT content generated by deepseek-reasoner, enabling them to view, display, and distill it."""
# enabled = false  # Optional: set to false to disable this specific model

[[providers.openai-like.models]]
id = "gemini-2.5-pro"
name = "gemini-2.5-pro"
max_tokens = 65535
context_window = 200000
description = """{"code": 88, "write": 85, "reasoning": 92, "multimodal": 95, "cost": "input $1.25 output $10.00", "info": "Google's flagship Gemini model with state-of-the-art multimodal integration, top LMArena scores, and strong mathematical reasoning. Supports 1 M+ token context and robust video understanding. Excels at complex multimodal research tasks; prompt design remains critical and resource usage is high."}"""


# -------------------------------------------------
# Example of a second third-party provider (currently disabled)
# -------------------------------------------------
[providers.openai-like-2]
id = "openai-like-2"
name = "openai-like-2"
family = "openai"
base_url = "https://example.com/v1"
api_key = ""
enabled = false
priority = 10
[[providers.openai-like-2.models]]
id = "gpt-4o"
name = "gpt-4o"
max_tokens = 16384
context_window = 128000
description = """This is a description of the model."""
# -------------------------------------------------
# How to add more providers
# -------------------------------------------------
# 1. Copy one of the providers blocks above.
# 2. Give it a unique [providers.<id>] section and fill base_url, api_key, enabled, priority.
# 3. Add one or more [[providers.<id>.models]] entries with id, name, max_tokens, context_window, description.
# 4. Set default_provider to whichever provider you want as the fallback.
# 5. Use priority to control fallback order (lower number = higher priority).
# 6. Set enabled = false on individual models to disable them without removing the provider.
# 7. Models have enabled = true by default, so you only need to specify if you want to disable them.
# That's allâ€”repeat as needed to support any number of LLM back-ends.